{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CALuVmNMNkM"
   },
   "source": [
    "# Autoencoder for MNIST\n",
    "In this notebook you will train an autoencoder for the MNIST dataset which is a dataset of handwritten digits. This is the last exercise where we will provide a structured skeleton. For future exercises, we will only provide the dataset, task as well as a test scenario for you to challenge yourself against your peers on our leaderboards.\n",
    "\n",
    "## What we will do:\n",
    "\n",
    "One application of autoencoders is unsupervised pretraining with unlabeled data and then finetuning the encoder with labeled data. This can increase our performance if there is only little labeled data but a lot of unlabeled data available.\n",
    "\n",
    "In this exercise you use the MNIST dataset with 60,000 images of handwritten digits, but you do not have all the labels available.\n",
    "\n",
    "You will then train our autoencoder to reproduce the unlabeled images. \n",
    "\n",
    "Then you will transfer the pretrained encoder weights and finetune a classifier on the labeled data for classifying the handwritten digits. This is called **transfer learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "XcU9f4APMNkT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# For automatic file reloading as usual\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xb9dFU2EMNkW"
   },
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment the following cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "TRr4E4YVMNkW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ngdrive_path='/content/gdrive/MyDrive/i2dl/exercise_08'\\n\\n# This will mount your google drive under 'MyDrive'\\ndrive.mount('/content/gdrive', force_remount=True)\\n# In order to access the files in this notebook we have to navigate to the correct folder\\nos.chdir(gdrive_path)\\n# Check manually if all files are present\\n\""
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_08) is given.\n",
    "\n",
    "# from google.colab import drive\n",
    "# import os\n",
    "\n",
    "\"\"\"\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_08'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzDQg-kDMNkY"
   },
   "source": [
    "### Set up PyTorch environment in colab\n",
    "- (OPTIONAL) Enable GPU via Runtime --> Change runtime type --> GPU\n",
    "- Uncomment the following cell if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "pIUdsXeXMNkZ"
   },
   "outputs": [],
   "source": [
    "# Optional: install correct libraries in google colab\n",
    "# !python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchtext==0.12.0+cu113 torchaudio==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !python -m pip install tensorboard==2.8.0 > /dev/null\n",
    "# !python -m pip install pytorch-lightning==1.6.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEDWAZ7-ZA4E"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "dJCiVLV5o9QO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efd4d165cd0>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from exercise_code.image_folder_dataset import ImageFolderDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvaj6myXS7nN"
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note: Google Colab</h3>\n",
    "    <p>\n",
    "In case you don't have a GPU, you can run this notebook on Google Colab where you can access a GPU for free, but, of course, you can also run this notebook on your CPU.\n",
    "         </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "VWgm75NnS9hr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm_rTAPnpsUo"
   },
   "source": [
    "## Setup TensorBoard\n",
    "In exercise 07 you've already learned how to use TensorBoard. Let's use it again to make the debugging of our network and training process more convenient! Throughout this notebook, feel free to add further logs or visualizations to your TensorBoard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "QbAJFyHkMNke"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 621), started 1:31:00 ago. (Use '!kill 621' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d36a064ccba88e8a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d36a064ccba88e8a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-Yt2KRiMNkf"
   },
   "source": [
    "# 1. The MNIST Dataset\n",
    "\n",
    "First, you download the dataset. MNIST is a dataset of 60,000 images depicting handwritten digits. However, as with most datasets, labeling is a costly process and therefore we are left in a pickle.\n",
    "\n",
    "A good starting point is to label a small subset of your images. You either do this yourself but in this instance we consider the case where you hired another student to do it for you. After writing a labeling tool and some time, you are provided with 300 labeled images of which 100 will be used for training, 100 for validation, and 100 for testing. A problematic small number...\n",
    "\n",
    "Feel free to define some transforms now or later (you can also pass without any transforms).\n",
    "\n",
    "**Note**: We do **NOT** apply any transformations to test set at the time of final evaluation on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "U5_eopjbMNkf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dataset folder. Skipped downloading. If you face issues, please re-download the dataset using\n",
      "'--force_download=True'\n",
      "https://i2dl.vc.in.tum.de/static/data/mnist.zip\n",
      "Found dataset folder. Skipped downloading. If you face issues, please re-download the dataset using\n",
      "'--force_download=True'\n",
      "https://i2dl.vc.in.tum.de/static/data/mnist.zip\n",
      "Found dataset folder. Skipped downloading. If you face issues, please re-download the dataset using\n",
      "'--force_download=True'\n",
      "https://i2dl.vc.in.tum.de/static/data/mnist.zip\n",
      "Found dataset folder. Skipped downloading. If you face issues, please re-download the dataset using\n",
      "'--force_download=True'\n",
      "https://i2dl.vc.in.tum.de/static/data/mnist.zip\n",
      "Found dataset folder. Skipped downloading. If you face issues, please re-download the dataset using\n",
      "'--force_download=True'\n",
      "https://i2dl.vc.in.tum.de/static/data/mnist.zip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose([])\n",
    "########################################################################\n",
    "# TODO: Feel free to define transforms                                 #\n",
    "########################################################################\n",
    "\n",
    "transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                transforms.RandomAffine(0,shear=30,scale=[1.15,1.15]),\n",
    "                                transforms.RandomRotation(10),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307), (0.3081))]\n",
    "                              )\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################\n",
    "\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "mnist_root = os.path.join(i2dl_exercises_path, \"datasets\", \"mnist\")\n",
    "\n",
    "train_100_dataset = ImageFolderDataset(root=mnist_root,images='train_images.pt',labels='train_labels.pt',force_download=False,verbose=True,transform=transform)\n",
    "val_100_dataset = ImageFolderDataset(root=mnist_root,images='val_images.pt',labels='val_labels.pt',force_download=False,verbose=True,transform=transform)\n",
    "test_100_dataset = ImageFolderDataset(root=mnist_root,images='test_images.pt',labels='test_labels.pt',force_download=False,verbose=True,transform=transform)\n",
    "\n",
    "# We also set up the unlabeled images which we will use later\n",
    "unlabeled_train = ImageFolderDataset(root=mnist_root,images='unlabeled_train_images.pt',force_download=False,verbose=True,transform=transform)\n",
    "unlabeled_val = ImageFolderDataset(root=mnist_root,images='unlabeled_val_images.pt',force_download=False,verbose=True,transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwrT1ckAMNkg"
   },
   "source": [
    "The dataset consists of tuples of 28x28 pixel PIL images and a label that is an integer from 0 to 9. \n",
    "\n",
    "Let's turn a few of the images into numpy arrays, to look at their shape and visualize them and see if the labels we paid for are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "k7ct1J2CMNkh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our greyscale images:  (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x1600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (16,16) # Make the figures a bit bigger\n",
    "\n",
    "indices_arr = [83, 98, 92, 99, 78, 97, 90, 95, 93, 96]\n",
    "for i, index in enumerate(indices_arr):\n",
    "    image = np.array(train_100_dataset[index][0].squeeze()) # get the image of the data sample\n",
    "    label = train_100_dataset[index][1] # get the label of the data sample\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.imshow(image, cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(label))\n",
    "    \n",
    "plt.tight_layout()\n",
    "print('The shape of our greyscale images: ', image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9sz_lHyqJoj"
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note: Starting Simple</h3>\n",
    "    <p>\n",
    "Regardless of the size of our dataset, the first step we have to take is to evaluate the performance of a simple classifier. Always approach a problem with a simple approach first and go from there to see which changes are helping you.\n",
    "         </p>\n",
    "</div>\n",
    "\n",
    "# 2. A Simple Classifier\n",
    "\n",
    "In `exercise_code/models.py` we prepared all classes for you which you will finalize throughout the notebook to build an Autoencoder and an image classifier with PyTorch.\n",
    "\n",
    "![network_split](img/network_split.png)\n",
    "\n",
    "## 2.1 The Encoder\n",
    "\n",
    "Different to previous models, we are going to split up our model into two parts: the so called `encoder` and the `classifier`. The `classifier` has a static task as it will output our predictions given a one-dimensional input. The `encoder`'s task is to extract meaningful information out of our input so that the classifier can make a proper decision. Right now however, both networks will be consisting of linear layers coupled with auxiliary ones and therefore won't be too different in their design. This split-up will be relevant later, e.g., by using convolutional layers which are introduced in the lecture. We are going to set up the `encoder` now. \n",
    "\n",
    "Think about a good network architecture. You're completely free here and can come up with any network you like! (\\*)\n",
    "\n",
    "Have a look at the documentation of `torch.nn` at https://pytorch.org/docs/stable/nn.html to learn how to use this module to build your network!\n",
    "\n",
    "Then implement your architecture: initialize it in `__init__()` and assign it to `self.model`. This is particularly easy using `nn.Sequential()` which you only have to pass the list of your layers. \n",
    "\n",
    "To make your model customizable and support parameter search, don't use hardcoded hyperparameters - instead, pass them as dictionary `hparams` (here, `n_hidden` is the number of neurons in the hidden layer) when initializing `models`.\n",
    "\n",
    "Here's an simple example:\n",
    "\n",
    "```python\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, self.hparams[\"n_hidden\"]),\n",
    "            nn.ReLU(),            \n",
    "            nn.Linear(self.hparams[\"n_hidden\"], num_classes)\n",
    "        )\n",
    "```\n",
    "\n",
    "Have a look at the forward path in `forward(self, x)`, which is so easy that you don't need to implement it yourself.\n",
    "\n",
    "As PyTorch automatically computes the gradients, that's all we need to do! No need to manually calculate derivatives for the backward paths anymore! :)\n",
    "\n",
    "\n",
    "____\n",
    "\\* *The size of your final model must be less than 20 MB, which is approximately equivalent to 5 Mio. params. Note that this limit is quite lenient, you will probably need much less parameters!*\n",
    "\n",
    "*In order to keep things simple, you should only use fully connected layers for this task as we need to revert the encoder architecture  later on in the notebook.*\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>Encoder</code> class initialization in <code>exercise_code/models.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNf7FrvwMNki"
   },
   "source": [
    "## 2.2 The Classifier\n",
    "\n",
    "Now we are implementing our classifier. It will use the encoder network that you have defined in the above cell. By looking at `Classifier.forward`, you can see that we are simply chaining the `classifier` as well as the `encoder` together. Therefore, you have to match the input shape of the classifier to the output shape of your encoder implemented above. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>1. Implement the <code>Classifier</code> class network initialization in <code>exercise_code/models.py</code>.\n",
    "    </p>\n",
    "    <p>2. Define in the next cell your hyperparameters in a dictionary called 'hparams'.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "AawbvD1rMNkj"
   },
   "outputs": [],
   "source": [
    "hparams = {}\n",
    "########################################################################\n",
    "# TODO: Define your hyper parameters here!                             #\n",
    "########################################################################\n",
    "\n",
    "hparams = {\n",
    "            \"epochs\": 100,\n",
    "            \"batch_size\": 20,\n",
    "            \"hidden_dim\": 100,\n",
    "            \"device\": torch.device(\"cuda\"),\n",
    "            \"lr\": 0.01\n",
    "          }\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOYbUg8lAmgU"
   },
   "source": [
    "\n",
    "## 2.3 Optimizer\n",
    "Lastly, implement the function `set_optimizer` to define your optimizer. Here the documentation of `torch.optim` at https://pytorch.org/docs/stable/optim.html might be helpful.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>set_optimizer</code> method of the <code>Classifier</code> in <code>exercise_code/models.py</code>.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrUfa-a7MNkk"
   },
   "source": [
    "## 2.4 Training & Validation Step\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p> Have a look at the following training pipeline. We explictly write it here fully, so you could learn how it should look like, and also refrence it when in doubt.\n",
    " </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "NY_lLaNWMNkk"
   },
   "outputs": [],
   "source": [
    " # One of the most crucial things in deep learning is to understand the training pipeline:\n",
    " # 1. Forward()          --> The forward pass of the network, to calculate the currnent loss.\n",
    " # 2. Backward()         --> The backward pass of the network, to calculate the gradients w.r.t the loss, calculated in the previous stage.\n",
    " # 3. Optimizer_step()   --> Update the weights w.r.t thier corresponding gradients and the learnign rate.\n",
    "\n",
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable),total=len(iterable), ncols=150, desc=desc, leave=False)\n",
    "\n",
    "\n",
    "def train_classifier(classifier, train_loader, val_loader, loss_func, tb_logger, epochs=10, name=\"default\"):\n",
    "    \"\"\"\n",
    "    Train the classifier for a number of epochs.\n",
    "    \"\"\"\n",
    "    optimizer = classifier.optimizer\n",
    "    training_loss = 0\n",
    "    validation_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Training stage, where we want to update the parameters.\n",
    "        classifier.train()  # Set the model to training mode\n",
    "        \n",
    "        # Create a progress bar for the training loop.\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch}/{epochs}]')\n",
    "        for train_iteration, batch in training_loop:\n",
    "            optimizer.zero_grad() # Reset the gradients - VERY important! Otherwise they accumulate.\n",
    "            images, labels = batch # Get the images and labels from the batch, in the fashion we defined in the dataset and dataloader.\n",
    "            images, labels = images.to(device), labels.to(device) # Send the data to the device (GPU or CPU) - it has to be the same device as the model.\n",
    "\n",
    "            # Flatten the images to a vector. This is done because the classifier expects a vector as input.\n",
    "            # Could also be done by reshaping the images in the dataset.\n",
    "            images = images.view(images.shape[0], -1) \n",
    "\n",
    "            pred = classifier(images) # Stage 1: Forward().\n",
    "            loss = loss_func(pred, labels) # Compute the loss over the predictions and the ground truth.\n",
    "            loss.backward()  # Stage 2: Backward().\n",
    "            optimizer.step() # Stage 3: Update the parameters.\n",
    "\n",
    "            if epoch == epochs - 1: # Save the last epoch's loss.\n",
    "                training_loss += loss.item()\n",
    "\n",
    "            # Update the progress bar.\n",
    "            training_loop.set_postfix(curr_train_loss = \"{:.8f}\".format(loss.item()), val_loss = \"{:.8f}\".format(validation_loss))\n",
    "\n",
    "            # Update the tensorboard logger.\n",
    "            tb_logger.add_scalar(f'classifier_{name}/train_loss', loss.item(), epoch * len(train_loader) + train_iteration)\n",
    "            sleep(0.1) # Remove this line if you want to see the progress bar faster.\n",
    "\n",
    "        # Validation stage, where we don't want to update the parameters. Pay attention to the classifier.eval() line\n",
    "        # and \"with torch.no_grad()\" wrapper.\n",
    "        classifier.eval()\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Epoch [{epoch}/{epochs}]')\n",
    "        validation_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                images, labels = batch\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                images = images.view(images.shape[0], -1) \n",
    "                pred = classifier(images)\n",
    "                loss = loss_func(pred, labels)\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "                # Update the progress bar.\n",
    "                val_loop.set_postfix(curr_val_loss = \"{:.8f}\".format(loss.item()))\n",
    "\n",
    "                # Update the tensorboard logger.\n",
    "                tb_logger.add_scalar(f'classifier_{name}/val_loss', loss.item(), epoch * len(val_loader) + val_iteration)\n",
    "                sleep(0.1) # Remove this line if you want to see the progress bar faster.\n",
    "        \n",
    "        # This value is used for the progress bar of the training loop.\n",
    "        validation_loss /= len(val_loader)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVKLlwlyMNkl"
   },
   "source": [
    "## 2.5 Fit Classification Model with Trainer\n",
    "Now it's time to train your model.\n",
    "Run the following cell to see the behold the magic of deep learning at play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "uBGavq9cMNkl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "How did we do? Let's check the accuracy of the defaut classifier on the training and validation sets:\n",
      "Training Acc: 100.0%\n",
      "Validation Acc: 65.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from exercise_code.models import Classifier\n",
    "from exercise_code.models import Encoder\n",
    "\n",
    "# Create the encoder and the classifier.\n",
    "encoder = Encoder(hparams).to(device)\n",
    "classifier = Classifier(hparams, encoder).to(device)\n",
    "\n",
    "# Creat a tensorboard logger.\n",
    "# NOTE: In order to see the logs, run the following command in the terminal: tensorboard --logdir=./\n",
    "# Also, in order to reset the logs, delete the logs folder MANUALLY.\n",
    "\n",
    "path = os.path.join('logs', 'cls_logs')\n",
    "num_of_runs = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "path = os.path.join(path, f'run_{num_of_runs + 1}')\n",
    "\n",
    "tb_logger = SummaryWriter(path)\n",
    "\n",
    "# Train the classifier.\n",
    "labled_train_loader = torch.utils.data.DataLoader(train_100_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
    "labled_val_loader = torch.utils.data.DataLoader(val_100_dataset, batch_size=hparams['batch_size'], shuffle=False)\n",
    "\n",
    "epochs = hparams.get('epochs', 10)\n",
    "loss_func = nn.CrossEntropyLoss() # The loss function we use for classification.\n",
    "train_classifier(classifier, labled_train_loader, labled_val_loader, loss_func, tb_logger, epochs=epochs, name=\"Default\")\n",
    "\n",
    "print(\"Finished training!\")\n",
    "print(\"How did we do? Let's check the accuracy of the defaut classifier on the training and validation sets:\")\n",
    "print(f\"Training Acc: {classifier.getAcc(labled_train_loader)[1] * 100}%\")\n",
    "print(f\"Validation Acc: {classifier.getAcc(labled_val_loader)[1] * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i16vmHZXMNkm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Autoencoder\n",
    "\n",
    "One hundred images as training data are not much. How could we improve our performance with limited data? We have no money left to pay our student for more labels, and labeling the data ourselves is out of question. A good idea would be to do data augmentation to get the most out of our few labeled instances, but here we provide another way to solve this problem: we will use our large amount of unlabeled data to do unsupervised pretraining with an autoencoder, and then transfer the weights of our encoder to our classifier.\n",
    "\n",
    "For each image input, the autoencoder just tries to reproduce the same image as output. The difficulty behind is that the autoencoder has to go through a low dimensional bottleneck, which we call the **latent space**.\n",
    "In other words, the autoencoder should learn to represent all the input information in the low dimensional latent space; it learns to compress the input distribution.\n",
    "To make our model learn to reproduce the input, we use the mean squared error between our input pixels and the\n",
    "output pixels as the loss function. For this loss we do not need any labels!\n",
    "\n",
    "![autoencoder](img/autoencoder.png)\n",
    "\n",
    "After this, our encoder has learned to extract meaningful information from the inputs. We can then transfer its weights\n",
    "to a classifier architecture and finetune it with our labeled data, i.e., instead of initializing our encoder randomly we are re-using the weights of our trained encoder from our autoencoder network. This process is called **transfer learning**.\n",
    "\n",
    "![autoencoder_pretrained](img/pretrained.png)\n",
    "\n",
    "## 3.1 Decoder\n",
    "\n",
    "Before we can train our autoencoder, you have to initialize the your `decoder` architecture. The simplest way is to mirror your encoder architecture which ensure that the `latent space` output of our `encoder` is correctly transformed to our input shape.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>Decoder</code> and <code>Autoencoder</code> class initialization in <code>exercise_code/models.py</code>.</p>\n",
    "    <p>Implemet <code>training_step</code> and <code>validation_step</code> of the autoencoder, following the pipeline we've shown you in train_classifier().</p>\n",
    "    <p>Note the differences between the classificaiton task, and now the regression task!</p>\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "## 3.2 Autoencoder Training\n",
    "\n",
    "Now, we can train the full autoencoder consisting of both en- and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqqdoLDgMNkm"
   },
   "outputs": [],
   "source": [
    "from exercise_code.models import Autoencoder, Encoder, Decoder\n",
    "\n",
    "########################################################################\n",
    "# TODO: Define your hyperparameters here!                              #\n",
    "# Hint: use a large batch_size                                         #\n",
    "########################################################################\n",
    "\n",
    "hparams = {\n",
    "            \"epochs\": 100,\n",
    "            \"batch_size\": 4096,\n",
    "            \"hidden_dim\": 200,\n",
    "            \"device\": torch.device(\"cuda\"),\n",
    "            \"lr\": 0.01\n",
    "          }\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################\n",
    "encoder_pretrained = Encoder(hparams).to(device)\n",
    "decoder = Decoder(hparams).to(device)\n",
    "autoencoder = Autoencoder(hparams, encoder_pretrained, decoder).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRuIIm8YMNkn"
   },
   "source": [
    "Some tests to check whether we'll accept your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SoAaC-NqMNkn"
   },
   "outputs": [],
   "source": [
    "from exercise_code.Util import printModelInfo, load_model\n",
    "_ = printModelInfo(autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plQwnphtqggl"
   },
   "source": [
    "After implementing the relevant functions - read the following code, and then run it.\n",
    "Keep in mind that an epoch here will take much longer since\n",
    "we are iterating through 5,8600 images instead of just 100.\n",
    "\n",
    "For speed, colab is indeed recommended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uuzXMq6zjbb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [50/100]:  80%|███████████████████████████████▏       | 12/15 [00:10<00:02,  1.18it/s, curr_train_loss=0.84219164, val_loss=0.82916796]"
     ]
    }
   ],
   "source": [
    "encoder_pretrained = Encoder(hparams).to(device)\n",
    "decoder = Decoder(hparams).to(device)\n",
    "autoencoder = Autoencoder(hparams, encoder_pretrained, decoder).to(device)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_func, tb_logger, epochs=10, name='Autoencoder'):\n",
    "    \n",
    "    optimizer = model.optimizer\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=epochs * len(train_loader) / 5, gamma=0.7)\n",
    "    validation_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Train\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch}/{epochs}]')\n",
    "        for train_iteration, batch in training_loop:\n",
    "            \n",
    "            loss = model.training_step(batch, loss_func) # You need to implement this function.\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update the progress bar.\n",
    "            training_loop.set_postfix(curr_train_loss = \"{:.8f}\".format(loss.item()), val_loss = \"{:.8f}\".format(validation_loss))\n",
    "\n",
    "            # Update the tensorboard logger.\n",
    "            tb_logger.add_scalar(f'{name}/train_loss', loss.item(), epoch * len(train_loader) + train_iteration)\n",
    "\n",
    "        # Validation\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Epoch [{epoch}/{epochs}]')\n",
    "        validation_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                loss = model.validation_step(batch, loss_func) # You need to implement this function.\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "                # Update the progress bar.\n",
    "                val_loop.set_postfix(curr_val_loss = \"{:.8f}\".format(loss.item()))\n",
    "\n",
    "                # Update the tensorboard logger.\n",
    "                tb_logger.add_scalar(f'{name}/val_loss', loss.item(), epoch * len(val_loader) + val_iteration)\n",
    "\n",
    "        # This value is for the progress bar of the training loop.\n",
    "        validation_loss /= len(val_loader)\n",
    "\n",
    "# Creat a tensorboard logger.\n",
    "# NOTE: In order to see the logs, run the following command in the terminal: tensorboard --logdir=./\n",
    "# Also, in order to reset the logs, delete the logs folder MANUALLY.\n",
    "\n",
    "path = os.path.join('logs', 'ae_logs')\n",
    "num_of_runs = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "path = os.path.join(path, f'run_{num_of_runs + 1}')\n",
    "tb_logger = SummaryWriter(path)\n",
    "\n",
    "# Train the classifier.\n",
    "unlabled_train_loader = torch.utils.data.DataLoader(unlabeled_train, batch_size=hparams['batch_size'], shuffle=True)\n",
    "unlabled_val_loader = torch.utils.data.DataLoader(unlabeled_val, batch_size=hparams['batch_size'], shuffle=False)\n",
    "\n",
    "epochs = hparams.get('epochs', 5)\n",
    "loss_func = nn.MSELoss() # The loss function we use for regression (Could also be nn.L1Loss()).\n",
    "train_model(autoencoder, unlabled_train_loader, unlabled_val_loader, loss_func, tb_logger, epochs=epochs, name='Autoencoder')\n",
    "\n",
    "print(\"Finished training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdgiYWy4MNkq"
   },
   "source": [
    "Once trained, let's have a look at the reconstructed validation images (If you have not already looked at them in TensorBoard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a991mKcyMNkq"
   },
   "outputs": [],
   "source": [
    "reconstructions = autoencoder.getReconstructions(unlabled_val_loader)\n",
    "for i in range(64):\n",
    "    plt.subplot(8,8,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(reconstructions[i], cmap='gray', interpolation='none')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2hrP5b1MNkr"
   },
   "source": [
    "# 4. Transfer Learning\n",
    "\n",
    "## 4.1 The pretrained Classifier\n",
    "\n",
    "Now we initialize another classifier but this time with the pretrained encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OELYQAUmMNkr"
   },
   "outputs": [],
   "source": [
    "from exercise_code.models import Classifier\n",
    "from copy import deepcopy\n",
    "\n",
    "hparams = {}\n",
    "########################################################################\n",
    "# TODO: Define your hyper parameters here!                             #\n",
    "########################################################################\n",
    "\n",
    "hparams = {\n",
    "            \"epochs\": 100,\n",
    "            \"batch_size\": 25,\n",
    "            \"hidden_dim\": 200,\n",
    "            \"device\": torch.device(\"cuda\"),\n",
    "            \"lr\": 0.01\n",
    "          }\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################\n",
    "\n",
    "encoder_pretrained_copy = deepcopy(encoder_pretrained)\n",
    "classifier_pretrained = Classifier(hparams, encoder_pretrained_copy).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8FUtih6MNks"
   },
   "source": [
    "Now specify another trainer that we will use the pretrained classifier to compare its performance with\n",
    "the classifier we trained on only the labeled data. You might need to optimize the parameters defined above in order to achieve a reasonable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mx_euorWMNks"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Creat a tensorboard logger.\n",
    "# NOTE: In order to see the logs, run the following command in the terminal: tensorboard --logdir=./\n",
    "# Also, in order to reset the logs, delete the logs folder MANUALLY.\n",
    "# Pay attention that if you run this cell mutltiple times, the pretrained_encoder\n",
    "# is not reset, and will keep training from where it stopped. Thus, it could overfit.\n",
    "\n",
    "path = os.path.join('logs', 'pretrained_cls_logs')\n",
    "num_of_runs = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "path = os.path.join(path, f'run_{num_of_runs + 1}')\n",
    "tb_logger = SummaryWriter(path)\n",
    "\n",
    "batch_size = hparams.get('batch_size', 16)\n",
    "labled_train_loader = torch.utils.data.DataLoader(train_100_dataset, batch_size=batch_size, shuffle=True)\n",
    "labled_val_loader = torch.utils.data.DataLoader(val_100_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "epochs = hparams.get('epochs', 20)\n",
    "loss_func = nn.CrossEntropyLoss() # The loss function we use for classification.\n",
    "train_classifier(classifier_pretrained, labled_train_loader, labled_val_loader, loss_func, tb_logger, epochs=epochs, name='Pretrained')\n",
    "\n",
    "print(\"Finished training!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-pm1MY_MNks"
   },
   "source": [
    "Let's have a look at the validation accuracy of the two different classifiers and compare them. And don't forget that you can also monitor your training in TensorBoard.\n",
    "\n",
    "We will only look at the test accuracy and compare our two classifiers with respect to that in the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-e5Bd9KLMNkt"
   },
   "outputs": [],
   "source": [
    "print(\"Validation accuracy when training from scratch: {}%\".format(classifier.getAcc(labled_val_loader)[1]*100))\n",
    "print(\"Validation accuracy with pretraining: {}%\".format(classifier_pretrained.getAcc(labled_val_loader)[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAp2OTyf4_5b"
   },
   "source": [
    "Now that everything is working, feel free to play around with different architectures. As you've seen, it's really easy to define your model or do changes there.\n",
    "\n",
    "To pass this submission, you'll need an accuracy of **55%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmEYmRT-5S-e"
   },
   "source": [
    "# Save your model & Report Test Accuracy\n",
    "\n",
    "When you've done with your **hyperparameter tuning**, have achieved **at least 55% validation accuracy** and are happy with your final model, you can save it here.\n",
    "\n",
    "Before that, please check again whether the number of parameters is below 5 Mio and the file size is below 20 MB.\n",
    "\n",
    "When your final model is saved, we'll lastly report the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S69ETKxD5TcE"
   },
   "outputs": [],
   "source": [
    "from exercise_code.Util import test_and_save\n",
    "test_dl = torch.utils.data.DataLoader(test_100_dataset, batch_size=4, shuffle=False)\n",
    "print(\"Test accuracy when training from scratch: {}%\".format(classifier.getAcc(test_dl)[1]*100))\n",
    "print('\\nNow to the pretrained classifier:')\n",
    "test_and_save(classifier_pretrained, labled_val_loader, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enZCnGL6MNkt"
   },
   "outputs": [],
   "source": [
    "# Now zip the folder for upload\n",
    "from exercise_code.submit import submit_exercise\n",
    "\n",
    "submit_exercise('../output/exercise_08')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fuo3Tf9MNku",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Congrats! You've now finished your first autoencoder and transferred the weights to a classifier! Much easier than in plain numpy, right? But wait, to complete the exercise, submit your final model to [our submission portal](https://i2dl.dvl.in.tum.de/) - you should be already familiar with the procedure. Next, it is time to get started with some more complex neural networks and tasks in the upcoming exercises. See you next week!\n",
    "\n",
    "# Submission Goals\n",
    "\n",
    "- Goal: Successfully implement a fully connected autoencoder for MNIST with Pytorch Lightning and transfer the encoder weights to a classifier.\n",
    "\n",
    "- Passing Criteria: There are no unit tests that check specific components of your code. The only thing that's required to pass the submission, is your model to reach at least **55% accuracy** on __our__ test dataset. The submission system will show you a number between 0 and 100 which corresponds to your accuracy.\n",
    "\n",
    "- Submission start: __December 15, 2022 13.00__\n",
    "- Submission deadline : __December 21, 2022 15.59__ \n",
    "- You can make **$\\infty$** submissions until the deadline. Your __best submission__ will be considered for bonus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ar26mFO5MNku"
   },
   "source": [
    "# [Exercise Review](https://forms.gle/9SYivCPQZdktRDS29)\n",
    "\n",
    "We are always interested in your opinion. Now that you have finished this exercise, we would like you to give us some feedback about the time required to finish the submission and/or work through the notebooks. Please take the short time to fill out our [review form](https://forms.gle/9SYivCPQZdktRDS29) for this exercise so that we can do better next time! :)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c2ece17232a812cdc7d1f83cec9755c59779f4a9fdd58c69824919b136082647"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
